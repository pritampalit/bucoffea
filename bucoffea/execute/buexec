#!/usr/bin/env python
import argparse
import math
import os
import shutil
from datetime import datetime
from multiprocessing.pool import Pool
import itertools

import concurrent
import uproot
from coffea import processor
from coffea.util import save

from bucoffea.execute.dataset_definitions import (files_from_ac,
                                                  files_from_das,
                                                  files_from_eos)
from bucoffea.helpers import bucoffea_path, vo_proxy_path, xrootd_format
from bucoffea.helpers.condor import condor_submit
from bucoffea.helpers.git import git_rev_parse, git_diff
from bucoffea.processor.executor import run_uproot_job_nanoaod
from bucoffea.helpers.deployment import pack_repo

import socket

pjoin = os.path.join

def choose_processor(args):
    if args.processor == 'monojet':
        from bucoffea.monojet import monojetProcessor
        return monojetProcessor
    elif args.processor == 'vbfhinv':
        from bucoffea.vbfhinv import vbfhinvProcessor
        return vbfhinvProcessor
    elif args.processor == 'pdfweight':
        from bucoffea.gen import pdfWeightProcessor
        return pdfWeightProcessor
    elif args.processor == 'lhev':
        from bucoffea.gen import lheVProcessor
        return lheVProcessor
    elif args.processor == 'purity':
        from bucoffea.photon_purity import photonPurityProcessor
        return photonPurityProcessor
    elif args.processor == 'sumw':
        from bucoffea.gen import mcSumwProcessor
        return mcSumwProcessor

def do_run(args):
    """Run the analysis locally."""
    # Run over all files associated to dataset
    if args.datasrc == 'das':
        fileset = files_from_das(regex=args.dataset)
    else:
        fileset = files_from_eos(regex=args.dataset)

    ndatasets = len(fileset)
    nfiles = sum([len(x) for x in fileset.values()])
    print(f"Running over {ndatasets} datasets with a total of {nfiles} files.")
    for dataset, files in fileset.items():
        output = run_uproot_job_nanoaod({dataset:files},
                                    treename=args.tree,
                                    processor_instance=choose_processor(args)(),
                                    executor=processor.futures_executor,
                                    executor_args={'workers': args.jobs, 'flatten': True},
                                    chunksize=200000,
                                    )

        # Save output
        try:
            os.makedirs(args.outpath)
        except FileExistsError:
            pass
        outpath = pjoin(args.outpath, f"{args.processor}_{dataset}.coffea")
        save(output, outpath)

def do_worker(args):
    """Run the analysis on a worker node."""
    # Run over all files associated to dataset
    with open(args.filelist, "r") as f:
        files = [xrootd_format(x.strip()) for x in f.readlines()]
    fileset = {args.dataset : files}

    ndatasets = len(fileset)
    nfiles = sum([len(x) for x in fileset.values()])
    print(f"Running over {ndatasets} datasets with a total of {nfiles} files.")

    output = run_uproot_job_nanoaod(fileset,
                                  treename=args.tree,
                                  processor_instance=choose_processor(args)(),
                                  executor=processor.futures_executor,
                                  executor_args={'workers': args.jobs, 'flatten': True},
                                  chunksize=100000,
                                 )

    # Save output
    try:
        os.makedirs(args.outpath)
    except FileExistsError:
        pass
    outpath = pjoin(args.outpath, f"{args.processor}_{args.dataset}_{args.chunk}.coffea")
    save(output, outpath)



def chunk_by_files(items, nchunk):
    '''Split list of items into nchunk ~equal sized chunks'''
    chunks = [[] for _ in range(nchunk)]
    for i in range(len(items)):
        chunks[i % nchunk].append(items[i])
    return chunks

def chunk_by_events(filelist, chunksize=1e7, workers=4):
    executor = None if len(filelist) < 5 else concurrent.futures.ThreadPoolExecutor(workers)

    entries_per_file = sorted(
                            uproot.numentries(filelist, 'Events', total=False, executor=executor).items(),
                            key = lambda x: x[1]
                            )

    # Total number of events and number of chunks
    total = int(sum([x[1] for x in entries_per_file]))
    nchunks = int(max([total // chunksize, 1]))

    # Distribute files into chunks
    chunks = [[] for _ in range(nchunks)]

    # The files are sorted in ascending order by number of events,
    # so we add the highest-event file to a chunk, and then move
    # to the next chunk. When we get to the last chunk, do the
    # same thing, but moving backwards. Rinse + repeat until no more files.
    iter_index = itertools.cycle(itertools.chain(range(0, nchunks, 1), range(nchunks-1, -1, -1)))
    while len(entries_per_file):
        # Append to current chunk
        index = iter_index.__next__()
        item = entries_per_file.pop()
        chunks[index].append(item[0])

    # Sanity check
    all_files =[]
    for c in chunks:
        all_files.extend(c)
    assert(len(filelist)==len(all_files))
    assert(set(filelist)==set(all_files))

    return chunks

def do_submit(args):
    """Submit the analysis to HTCondor."""
    import htcondor


    if args.no_prefetch:
        print("WARNING: --no-prefetch is deprecated. Prefetching is disabled by default. Use --prefetch  if you want to turn it back on")

    if args.datasrc == 'das':
        dataset_files = files_from_das(regex=args.dataset)
    elif args.datasrc == 'ac':
        dataset_files = files_from_ac(regex=args.dataset)
    else:
        dataset_files = files_from_eos(regex=args.dataset)

    # Test mode: One file per data set
    if args.test:
        tmp = {}
        for k, v in dataset_files.items():
            tmp[k] = v[:1]
        dataset_files = tmp

    # Submission directory:
    # Uses tag from commandline if specified
    # Or just time tag
    timetag = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    if args.name:
        subdir = os.path.abspath(pjoin("./submission/", args.name))
        if os.path.exists(subdir) and not (args.force | args.append):
            raise RuntimeError(f"Will not use existing task directory unless '--force' or '--append' is specified: {subdir}")
    else:
        subdir = os.path.abspath(pjoin("./submission/", timetag))
    if not os.path.exists(subdir):
        os.makedirs(subdir)

    # Repo version information
    with open(pjoin(subdir, 'version.txt'),'w') as f:
        f.write(git_rev_parse()+'\n')
        f.write(git_diff()+'\n')

    # Sub-directory to store submission files
    filedir = 'files'
    if not os.path.exists(pjoin(subdir, filedir)):
        os.makedirs(pjoin(subdir, filedir))

    # Get proxy and copy to a safe location on AFS
    proxy = vo_proxy_path()
    proxydir = os.path.expanduser("~/.voms/")
    if not os.path.exists(proxydir):
        os.makedirs(proxydir)
    shutil.copy2(proxy, proxydir)

    if args.asynchronous:
        jdl_to_submit = []

    input_files = []
    if args.send_pack:
        gridpack_path = pjoin(subdir, 'gridpack.tgz')
        gridpack_exists = os.path.exists(gridpack_path)
        if (not gridpack_exists) or args.force:
            pack_repo(gridpack_path, overwrite=args.force)
        input_files.append(gridpack_path)

    for dataset, files in dataset_files.items():
        print(f"Writing submission files for dataset: {dataset}.")

        if args.filesperjob:
            nchunk = math.ceil(len(files)/args.filesperjob)
            chunks = chunk_by_files(files, nchunk=int(nchunk))
        else:
            chunks = chunk_by_events(files, chunksize=args.eventsperjob, workers=8)
        for ichunk, chunk in enumerate(chunks):
            # Save input files to a txt file and send to job
            tmpfile = pjoin(subdir, filedir, f"input_{dataset}_{ichunk:03d}of{len(chunks):03d}.txt")
            with open(tmpfile, "w") as f:
                for file in chunk:
                    f.write(f"{file}\n")

            # Job file creation
            arguments = [
                args.processor,
                f'--outpath .',
                f'--jobs {args.jobs}',
                f'--tree {args.tree}',
                'worker',
                f'--dataset {dataset}',
                f'--filelist {os.path.basename(tmpfile)}',
                f'--chunk {ichunk}'
            ]

            job_input_files = input_files + [
                os.path.abspath(tmpfile),
            ]


            environment = {
                "BUCOFFEAPREFETCH" : str(args.prefetch).lower()
            }
            if args.send_proxy:
                environment["X509_USER_PROXY"] = "$(Proxy_path)"
            if not args.send_pack:
                environment["VIRTUAL_ENV"] = os.environ["VIRTUAL_ENV"]
            if args.debug:
                environment['BUCOFFEADEBUG'] = 'true'

            chunkname = f'{dataset}_{ichunk:03d}of{len(chunks):03d}'
            submission_settings = {
                "Initialdir" : subdir,
                "executable": bucoffea_path("execute/htcondor_wrap.sh"),
                "should_transfer_files" : "YES",
                "when_to_transfer_output" : "ON_EXIT",
                "transfer_input_files" : ", ".join(job_input_files),
                "environment" : '"' + ' '.join([f"{k}={v}" for k, v in environment.items()]) + '"',
                "arguments": " ".join(arguments),
                "Output" : f"{filedir}/out_{chunkname}.txt",
                "Error" : f"{filedir}/err_{chunkname}.txt",
                "log" : f"{filedir}/log_{chunkname}.txt",
                # "log" :f"/dev/null",
                "request_cpus" : str(args.jobs),
                "request_memory" : str(args.memory if args.memory else args.jobs*2100),
                "+MaxRuntime" : f"{60*60*8}",
                "on_exit_remove" : "((ExitBySignal == False) && (ExitCode == 0)) || (NumJobStarts >= 2)",
                }
            if args.send_proxy:
                submission_settings['Proxy_path'] = pjoin(proxydir,os.path.basename(proxy))

            sub = htcondor.Submit(submission_settings)
            jdl = pjoin(subdir,filedir,f'job_{chunkname}.jdl')
            with open(jdl,"w") as f:
                f.write(str(sub))
                f.write("\nqueue 1\n")

            # Submission
            if args.dry:
                jobid = -1
                print(f"Submitted job {jobid}")
            else:
                if args.asynchronous:
                    jdl_to_submit.append(jdl)
                else:
                    jobid = condor_submit(jdl)
                    print(f"Submitted job {jobid}")
    if args.asynchronous:
        print('Starting asynchronous submission.')
        p = Pool(processes=8)
        res = p.map_async(condor_submit, jdl_to_submit)
        res.wait()
        if res.successful():
            print(f"Asynchronous submission successful for {len(jdl_to_submit)} jobs.")
        else:
            print("Asynchronous submission failed.")


def main():
    parser = argparse.ArgumentParser(prog='Execution wrapper for coffea analysis')
    parser.add_argument('processor', type=str, help='Processor to run.', choices=['monojet','vbfhinv','pdfweight','lhev','purity','sumw'])
    parser.add_argument('--outpath', type=str, help='Path to save output under.')
    parser.add_argument('--jobs','-j', type=int, default=1, help='Number of cores to use / request.')
    parser.add_argument('--datasrc', type=str, default='eos', help='Source of data files.', choices=['eos','das','ac'])
    parser.add_argument('--tree',type=str, default='Events', help='Name of the TTree in the input files.')

    subparsers = parser.add_subparsers(help='sub-command help')

    # Arguments passed to the "run" operation
    parser_run = subparsers.add_parser('run', help='Running help')
    parser_run.add_argument('--dataset', type=str, help='Dataset name to run over.')
    parser_run.set_defaults(func=do_run)

    # Arguments passed to the "worker" operation
    parser_run = subparsers.add_parser('worker', help='Running help')
    parser_run.add_argument('--dataset', type=str, help='Dataset name to run over.')
    parser_run.add_argument('--filelist', type=str, help='Text file with file names to run over.')
    parser_run.add_argument('--chunk', type=str, help='Number of this chunk for book keeping.')
    parser_run.set_defaults(func=do_worker)

    # Arguments passed to the "submit" operation
    parser_submit = subparsers.add_parser('submit', help='Submission help')
    parser_submit.add_argument('--dataset', type=str, help='Dataset regex to use.')
    parser_submit.add_argument('--filesperjob', type=int, default=None, help='Number of files to process per job')
    parser_submit.add_argument('--eventsperjob', type=int, default=5e6, help='Number of events to process per job')
    parser_submit.add_argument('--name', type=str, default=None, help='Name to identify this submission')
    parser_submit.add_argument('--prefetch', action="store_true", default=False, help='Prefetch input files on worker but run over xrootd.')
    parser_submit.add_argument('--no-prefetch', action="store_true", default=False, help='DEPRECATED. Prefetching is now disabled by default. Use --prefetch to activate prefetching.')
    parser_submit.add_argument('--dry', action="store_true", default=False, help='Do not trigger submission, just dry run.')
    parser_submit.add_argument('--test', action="store_true", default=False, help='Only run over one file per dataset for testing.')
    parser_submit.add_argument('--force', action="store_true", default=False, help='Overwrite existing submission folder and gridpack with same tag.')
    parser_submit.add_argument('--append', action="store_true", default=False, help='Submit in existing submission folder with same tag, keeping original gridpack.')
    parser_submit.add_argument('--asynchronous', action="store_true", default=False, help='Submit asynchronously.')
    parser_submit.add_argument('--async', action="store_true", default=False, help='Deprecated. Use --asynchronous instead.')
    parser_submit.add_argument('--debug', action="store_true", default=False, help='Print debugging info.')
    parser_submit.add_argument('--memory',type=int, default=None, help='Memory to request (in MB). Default is 2100 * number of cores.')
    parser_submit.set_defaults(func=do_submit)

    args = parser.parse_args()
    if getattr(args, 'async', False):
        raise IOError('The --async option is deprecated. Use --asynchronous instead.')

    host = socket.gethostname()
    if 'lpc' in host:
        args.send_pack = True
        args.send_proxy = False
    elif 'lxplus' in host:
        args.send_pack = True
        args.send_proxy = True

    args.func(args)



if __name__ == "__main__":
    main()
